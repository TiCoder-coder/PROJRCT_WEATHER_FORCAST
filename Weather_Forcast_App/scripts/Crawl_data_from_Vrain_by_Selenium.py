from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import pandas as pd
import time
import json
import os
import re
import unicodedata
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


class VrainCrawlerFinal:
    def __init__(self, headless=True, max_workers=5, max_retries=3):
        self.base_url = "https://www.vrain.vn"
        self.all_rainfall_data = []
        self.headless = headless
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.data_lock = threading.Lock()
        self.unique_stations = {}
        self.failed_provinces = []

    def create_driver(self):
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless=new")

        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_experimental_option(
            "prefs", {"profile.default_content_setting_values": {"images": 2}}
        )
        chrome_options.page_load_strategy = "eager"

        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        return driver

    def normalize_string(self, text):
        """Chuáº©n hÃ³a tiáº¿ng Viá»‡t vÃ  loáº¡i bá» khoáº£ng tráº¯ng thá»«a"""
        if not text:
            return ""
        text = unicodedata.normalize("NFC", text)
        return " ".join(text.strip().split())

    def extract_rainfall(self, text):
        """TrÃ­ch xuáº¥t sá»‘ tá»« chuá»—i '12.5 mm'"""
        match = re.search(r"(\d+[\.,]?\d*)", text)
        return match.group(1).replace(",", ".") if match else "0"

    def get_province_name(self, driver):
        """Láº¥y tÃªn tá»‰nh tá»« cÃ¡c selector khÃ¡c nhau"""
        selectors = [
            "span[_ngcontent-ng-c641299110]",
            "span[_ngcontent-serverapp-c641299110]",
            "span[class*='ng-']",
            ".app-title span",
            "h1 span",
            ".province-name",
            "h1",
            "title",
        ]

        for selector in selectors:
            try:
                element = driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 1:
                    text = re.sub(r"(VRAIN|LÆ°á»£ng mÆ°a.*|[-|])", "", text).strip()
                    if text and not any(
                        x in text.lower() for x in ["táº¡i cÃ¡c tráº¡m", "ngÃ y"]
                    ):
                        return self.normalize_string(text)
            except:
                continue

        try:
            title = driver.title
            if title and "VRAIN" in title:
                text = title.replace("VRAIN", "").replace("-", "").strip()
                if text:
                    return self.normalize_string(text)
        except:
            pass

        return None

    def crawl_province(self, province_id, retry_count=0):
        """Crawl má»™t tá»‰nh vá»›i cÆ¡ cháº¿ retry"""
        driver = None
        try:
            driver = self.create_driver()
            url = f"{self.base_url}/{province_id}/overview?public_map=windy"
            driver.get(url)

            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "body")))
            time.sleep(4)

            province_name = self.get_province_name(driver)

            if not province_name:
                province_name = f"ID_{province_id}"

            found_count = 0
            crawl_time = datetime.now().strftime("%d/%m/%Y %H:%M")

            selectors = [
                "div[class*='station-row']",
                "div[class*='station']",
                "tr.station-item",
                ".station-list-item",
                "table tbody tr",
                "table tr",
                ".data-row",
            ]

            elements = []
            for selector in selectors:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if len(elements) > 2:
                    break

            for el in elements:
                try:
                    text_content = el.text.strip()
                    if not text_content or "LÆ°á»£ng mÆ°a" in text_content:
                        continue

                    lines = [
                        line.strip()
                        for line in text_content.split("\n")
                        if line.strip()
                    ]

                    if len(lines) >= 2:
                        station_name = self.normalize_string(lines[0])
                        rainfall_val = self.extract_rainfall(lines[-1])

                        unique_key = f"{province_name}_{station_name}".lower()

                        with self.data_lock:
                            if unique_key not in self.unique_stations:
                                record = {
                                    "province_id": province_id,
                                    "tinh": province_name,
                                    "tram": station_name,
                                    "luong_mua": float(rainfall_val),
                                    "thoi_gian": crawl_time,
                                }
                                self.unique_stations[unique_key] = record
                                found_count += 1
                except:
                    continue

            if found_count == 0 or province_name.startswith("ID_"):
                if retry_count < self.max_retries:
                    print(
                        f"âš ï¸  ID {province_id}: {province_name} - KhÃ´ng cÃ³ dá»¯ liá»‡u, thá»­ láº¡i láº§n {retry_count + 1}..."
                    )
                    if driver:
                        driver.quit()
                    time.sleep(2)
                    return self.crawl_province(province_id, retry_count + 1)
                else:
                    print(
                        f"âŒ ID {province_id}: Tháº¥t báº¡i sau {self.max_retries} láº§n thá»­"
                    )
                    with self.data_lock:
                        self.failed_provinces.append(province_id)
                    return 0

            print(f"âœ… ID {province_id}: {province_name} - Láº¥y Ä‘Æ°á»£c {found_count} tráº¡m")
            return found_count

        except Exception as e:
            if retry_count < self.max_retries:
                print(
                    f"âš ï¸  ID {province_id} lá»—i: {str(e)[:30]} - Thá»­ láº¡i láº§n {retry_count + 1}..."
                )
                if driver:
                    driver.quit()
                time.sleep(2)
                return self.crawl_province(province_id, retry_count + 1)
            else:
                print(
                    f"âŒ Lá»—i ID {province_id} sau {self.max_retries} láº§n thá»­: {str(e)[:50]}"
                )
                with self.data_lock:
                    self.failed_provinces.append(province_id)
                return 0
        finally:
            if driver:
                driver.quit()

    def run(self, start_id=1, end_id=63):
        print(f"ðŸš€ Báº¯t Ä‘áº§u crawl tá»« ID {start_id} Ä‘áº¿n {end_id}...")
        print(f"ðŸ”„ Sá»‘ láº§n thá»­ láº¡i tá»‘i Ä‘a: {self.max_retries}")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            executor.map(self.crawl_province, range(start_id, end_id + 1))

        if self.failed_provinces:
            print(f"\nðŸ”„ Äang retry {len(self.failed_provinces)} tá»‰nh tháº¥t báº¡i...")
            retry_failed = []
            for province_id in self.failed_provinces:
                time.sleep(1)
                result = self.crawl_province(province_id, 0)
                if result == 0:
                    retry_failed.append(province_id)

            self.failed_provinces = retry_failed

        self.all_rainfall_data = list(self.unique_stations.values())
        self.all_rainfall_data.sort(key=lambda x: (x["province_id"], x["tram"]))

    def export(self):
        if not self.all_rainfall_data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xuáº¥t!")
            return

        df = pd.DataFrame(self.all_rainfall_data)
        df = df.drop(columns=["province_id"], errors="ignore")

        df = df.rename(columns={
            "tinh": "Tá»‰nh/ThÃ nh phá»‘",
            "tram": "TÃªn tráº¡m",
            "luong_mua": "Tá»•ng lÆ°á»£ng mÆ°a",
            "thoi_gian": "Thá»i gian cáº­p nháº­p",
        })


        output_dir = "/media/voanhnhat/SDD_OUTSIDE5/PROJECT_WEATHER_FORECAST/Weather_Forcast_App/output"
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_path = os.path.join(output_dir, f"Bao_cao_{timestamp}.xlsx")
        df.to_excel(excel_path, index=False)

        print(f"\n{'=' * 60}")
        print(f"ðŸ“Š Tá»”NG Káº¾T:")
        print(f"ðŸ“ Tá»•ng sá»‘ tráº¡m thu tháº­p: {len(df)}")

        if self.failed_provinces:
            print(
                f"âŒ CÃ¡c ID váº«n tháº¥t báº¡i: {', '.join(map(str, self.failed_provinces))}"
            )
        else:
            print(f"âœ… Táº¥t cáº£ tá»‰nh Ä‘á»u láº¥y dá»¯ liá»‡u thÃ nh cÃ´ng!")

        print(f"ðŸ“„ File Ä‘Ã£ lÆ°u táº¡i: {excel_path}")
        print(f"{'=' * 60}")


if __name__ == "__main__":
    crawler = VrainCrawlerFinal(headless=True, max_workers=3, max_retries=3)
    crawler.run(start_id=1, end_id=63)
    crawler.export()